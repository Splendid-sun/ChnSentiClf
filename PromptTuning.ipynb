{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5c78092f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "import random\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12968f02",
   "metadata": {},
   "source": [
    "# 设置随机种子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28cee952",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed=1029):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    \n",
    "seed_everything(12)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05971b59",
   "metadata": {},
   "source": [
    "# **1. 准备数据**\n",
    "## **构建数据集**\n",
    "与之前一样，我们首先编写继承自 `Dataset` 类的自定义数据集用于组织样本和标签。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d65e945b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from datasets import load_from_disk\n",
    "\n",
    "class ChnSentiCorp(Dataset):\n",
    "    def __init__(self, split):\n",
    "        self.dataset = load_from_disk('./data/ChnSentiCorp')[split]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.dataset[idx]\n",
    "\n",
    "train_data = ChnSentiCorp('train')\n",
    "valid_data = ChnSentiCorp('validation')\n",
    "test_data = ChnSentiCorp('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cf35f102",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': '非常不错，服务很好，位于市中心区，交通方便，不过价格也高！', 'label': 1}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b21b9c",
   "metadata": {},
   "source": [
    "下面我们输出数据集的尺寸，并且打印出一个训练样本："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ba01dc2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set size: 9600\n",
      "valid set size: 0\n",
      "test set size: 1200\n",
      "{'text': '选择珠江花园的原因就是方便，有电动扶梯直接到达海边，周围餐馆、食廊、商场、超市、摊位一应俱全。酒店装修一般，但还算整洁。 泳池在大堂的屋顶，因此很小，不过女儿倒是喜欢。 包的早餐是西式的，还算丰富。 服务吗，一般', 'label': 1}\n"
     ]
    }
   ],
   "source": [
    "print(f'train set size: {len(train_data)}')\n",
    "print(f'valid set size: {len(valid_data)}')\n",
    "print(f'test set size: {len(test_data)}')\n",
    "print(next(iter(train_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdead385",
   "metadata": {},
   "source": [
    "下面我们首先编写模板和 verbalizer 对应的函数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b831110c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "checkpoint = \"bert-base-chinese\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "def get_prompt(x):\n",
    "#prompt = f'总体上来说很[MASK]。{x}'\n",
    "    prompt = f'[unused1][unused2][unused3][unused4][MASK][unused5][unused6][unused7][unused8]。{x}'\n",
    "    return {\n",
    "        'prompt': prompt, \n",
    "        'mask_offset': prompt.find('[MASK]')\n",
    "    }\n",
    "\n",
    "def get_verbalizer(tokenizer):\n",
    "    return {\n",
    "        'pos': {'token': '好', 'id': tokenizer.convert_tokens_to_ids(\"好\")}, \n",
    "        'neg': {'token': '差', 'id': tokenizer.convert_tokens_to_ids(\"差\")}\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df12d2b2",
   "metadata": {},
   "source": [
    "例如，第一个样本转换后的模板为："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "572e3f57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "verbalizer: {'pos': {'token': '好', 'id': 1962}, 'neg': {'token': '差', 'id': 2345}}\n",
      "prompt: [unused1][unused2][unused3][unused4][MASK][unused5][unused6][unused7][unused8]。这个宾馆比较陈旧了，特价的房间也很一般。总体来说一般。\n",
      "prompt tokens: ['[CLS]', '[', 'u', '##nus', '##ed', '##1', ']', '[', 'u', '##nus', '##ed', '##2', ']', '[', 'u', '##nus', '##ed', '##3', ']', '[', 'u', '##nus', '##ed', '##4', ']', '[MASK]', '[', 'u', '##nus', '##ed', '##5', ']', '[', 'u', '##nus', '##ed', '##6', ']', '[', 'u', '##nus', '##ed', '##7', ']', '[', 'u', '##nus', '##ed', '##8', ']', '。', '这', '个', '宾', '馆', '比', '较', '陈', '旧', '了', '，', '特', '价', '的', '房', '间', '也', '很', '一', '般', '。', '总', '体', '来', '说', '一', '般', '。', '[SEP]']\n",
      "mask idx: 25\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "checkpoint = \"bert-base-chinese\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "comment = '这个宾馆比较陈旧了，特价的房间也很一般。总体来说一般。'\n",
    "\n",
    "print('verbalizer:', get_verbalizer(tokenizer))\n",
    "\n",
    "prompt_data = get_prompt(comment)\n",
    "prompt, mask_offset = prompt_data['prompt'], prompt_data['mask_offset']\n",
    "\n",
    "encoding = tokenizer(prompt, truncation=True)\n",
    "tokens = encoding.tokens()\n",
    "mask_idx = encoding.char_to_token(mask_offset)\n",
    "\n",
    "print('prompt:', prompt)\n",
    "print('prompt tokens:', tokens)\n",
    "print('mask idx:', mask_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e0fed80",
   "metadata": {},
   "source": [
    "这里我们可以为“积极”和“消极”构建专门的虚拟 token “[POS]”和“[NEG]”，并且设置对应的类别描述为“好的、优秀的、正面的评价、积极的态度”和“差的、糟糕的、负面的评价、消极的态度”。下面我们扩展一下上面的 `verbalizer` mmm函数，添加一个 `vtype` 参数来区分两种 `verbalizer` 类型：m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e7ec54dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "verbalizer: {'pos': {'token': '[POS]', 'id': 21128, 'description': '好的、优秀的、正面的评价、积极的态度'}, 'neg': {'token': '[NEG]', 'id': 21129, 'description': '差的、糟糕的、负面的评价、消极的态度'}}\n"
     ]
    }
   ],
   "source": [
    "def get_verbalizer(tokenizer, vtype):\n",
    "    assert vtype in ['base', 'virtual']\n",
    "    return {\n",
    "        'pos': {'token': '好', 'id': tokenizer.convert_tokens_to_ids(\"好\")}, \n",
    "        'neg': {'token': '差', 'id': tokenizer.convert_tokens_to_ids(\"差\")}\n",
    "    } if vtype == 'base' else {\n",
    "        'pos': {\n",
    "            'token': '[POS]', 'id': tokenizer.convert_tokens_to_ids(\"[POS]\"), \n",
    "            'description': '好的、优秀的、正面的评价、积极的态度'\n",
    "        }, \n",
    "        'neg': {\n",
    "            'token': '[NEG]', 'id': tokenizer.convert_tokens_to_ids(\"[NEG]\"), \n",
    "            'description': '差的、糟糕的、负面的评价、消极的态度'\n",
    "        }\n",
    "    }\n",
    "\n",
    "vtype = 'virtual'\n",
    "# add label words\n",
    "if vtype == 'virtual':\n",
    "    tokenizer.add_special_tokens({'additional_special_tokens': ['[POS]', '[NEG]']})\n",
    "print('verbalizer:', get_verbalizer(tokenizer, vtype=vtype))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a6364f",
   "metadata": {},
   "source": [
    "Prompting 方法实际输入的是转换后的模板，而不是原始文本，因此我们首先使用模板函数 `get_prompt()` 来更新数据集："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "02fc89b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(data):\n",
    "    data['comment'] = data['text']\n",
    "    prompt_data = get_prompt(data['text'])\n",
    "    data['prompt'] = prompt_data['prompt']\n",
    "    data['mask_offset'] = prompt_data['mask_offset']\n",
    "    return data\n",
    "    \n",
    "class ChnSentiCorp(Dataset):\n",
    "    def __init__(self, split):\n",
    "        self.data = load_from_disk('./data/ChnSentiCorp')[split]\n",
    "        self.data = self.data.map(f)\n",
    "        self.data = self.data.remove_columns(['text'])\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "    \n",
    "train_data = ChnSentiCorp('train')\n",
    "valid_data = ChnSentiCorp('validation')\n",
    "test_data = ChnSentiCorp('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "bacaceed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['label', 'comment', 'prompt', 'mask_offset'],\n",
       "    num_rows: 9600\n",
       "})"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "83f99823",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label': 0,\n",
       " 'comment': '我看过朋友的还可以，但是我订的书迟迟未到已有半个月，都没有收到打电话也没有用，以后你们订书一定要考虑好！当当实在是太慢了',\n",
       " 'prompt': '[unused1][unused2][unused3][unused4][MASK][unused5][unused6][unused7][unused8]。我看过朋友的还可以，但是我订的书迟迟未到已有半个月，都没有收到打电话也没有用，以后你们订书一定要考虑好！当当实在是太慢了',\n",
       " 'mask_offset': 36}"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.data[10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166c61a8",
   "metadata": {},
   "source": [
    "同样地，我们通过 `print(next(iter(train_data)))` 打印出一个训练样本："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "43071ad8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label': 1, 'comment': '选择珠江花园的原因就是方便，有电动扶梯直接到达海边，周围餐馆、食廊、商场、超市、摊位一应俱全。酒店装修一般，但还算整洁。 泳池在大堂的屋顶，因此很小，不过女儿倒是喜欢。 包的早餐是西式的，还算丰富。 服务吗，一般', 'prompt': '[unused1][unused2][unused3][unused4][MASK][unused5][unused6][unused7][unused8]。选择珠江花园的原因就是方便，有电动扶梯直接到达海边，周围餐馆、食廊、商场、超市、摊位一应俱全。酒店装修一般，但还算整洁。 泳池在大堂的屋顶，因此很小，不过女儿倒是喜欢。 包的早餐是西式的，还算丰富。 服务吗，一般', 'mask_offset': 36}\n"
     ]
    }
   ],
   "source": [
    "print(next(iter(train_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df74863d",
   "metadata": {},
   "source": [
    "## **数据预处理**\n",
    "\n",
    "与之前一样，接下来我们就通过 `DataLoader` 库来按批(batch)加载数据，将文本转换为模型可以接受的 token IDs。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "dc5a35fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_X shape: {'input_ids': torch.Size([8, 326]), 'token_type_ids': torch.Size([8, 326]), 'attention_mask': torch.Size([8, 326])}\n",
      "{'input_ids': tensor([[101, 138, 163,  ...,   0,   0,   0],\n",
      "        [101, 138, 163,  ...,   0,   0,   0],\n",
      "        [101, 138, 163,  ...,   0,   0,   0],\n",
      "        ...,\n",
      "        [101, 138, 163,  ...,   0,   0,   0],\n",
      "        [101, 138, 163,  ...,   0,   0,   0],\n",
      "        [101, 138, 163,  ...,   0,   0,   0]]), 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]])}\n",
      "[25, 25, 25, 25, 25, 25, 25, 25]\n",
      "[2345, 1962]\n",
      "[1, 0, 0, 0, 0, 1, 1, 0]\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "vtype = 'base'\n",
    "# vtype = 'virtual'\n",
    "max_length = 512\n",
    "batch_size = 8\n",
    "checkpoint = \"bert-base-chinese\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "if vtype == 'virtual':\n",
    "    tokenizer.add_special_tokens({'additional_special_tokens': ['[POS]', '[NEG]']})\n",
    "\n",
    "verbalizer = get_verbalizer(tokenizer, vtype=vtype)\n",
    "pos_id, neg_id = verbalizer['pos']['id'], verbalizer['neg']['id']\n",
    "\n",
    "def collote_fn(batch_samples):\n",
    "    batch_sentences, batch_mask_idxs, batch_labels  = [], [], []\n",
    "    for sample in batch_samples:\n",
    "        batch_sentences.append(sample['prompt'])\n",
    "        encoding = tokenizer(sample['prompt'], truncation=True)\n",
    "        mask_idx = encoding.char_to_token(sample['mask_offset'])\n",
    "        assert mask_idx is not None\n",
    "        batch_mask_idxs.append(mask_idx)\n",
    "        batch_labels.append(int(sample['label']))\n",
    "    batch_inputs = tokenizer(\n",
    "        batch_sentences, \n",
    "        max_length=max_length, \n",
    "        padding=True, \n",
    "        truncation=True, \n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    label_word_id = [neg_id, pos_id]\n",
    "    return {\n",
    "        'batch_inputs': batch_inputs, \n",
    "        'batch_mask_idxs': batch_mask_idxs, \n",
    "        'label_word_id': label_word_id, \n",
    "        'labels': batch_labels\n",
    "    }\n",
    "\n",
    "train_dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=True, collate_fn=collote_fn)\n",
    "valid_dataloader = DataLoader(valid_data, batch_size=batch_size, shuffle=False, collate_fn=collote_fn)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size, shuffle=False, collate_fn=collote_fn)\n",
    "\n",
    "batch_data = next(iter(train_dataloader))\n",
    "print('batch_X shape:', {k: v.shape for k, v in batch_data['batch_inputs'].items()})\n",
    "print(batch_data['batch_inputs'])\n",
    "print(batch_data['batch_mask_idxs'])\n",
    "print(batch_data['label_word_id'])\n",
    "print(batch_data['labels'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1746a64",
   "metadata": {},
   "source": [
    "# **2. 训练模型**\n",
    "\n",
    "## **构建模型**\n",
    "\n",
    "对于 MLM 任务，可以直接使用 Transformers 库封装好的 `AutoModelForMaskedLM` 类。由于 BERT 已经在 MLM 任务上进行了预训练，因此借助模板我们甚至可以在不微调的情况下 (Zero-shot) 直接使用模型来预测情感极性。例如对我们的第一个样本："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "5159a4e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForMaskedLM\n",
    "\n",
    "checkpoint = \"bert-base-chinese\"\n",
    "model = AutoModelForMaskedLM.from_pretrained(checkpoint)\n",
    "\n",
    "text = \"总体上来说很[MASK]。这个宾馆比较陈旧了，特价的房间也很一般。总体来说一般。\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "token_logits = model(**inputs).logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "6e6a26c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'>>> 总体上来说很好。这个宾馆比较陈旧了，特价的房间也很一般。总体来说一般。'\n",
      "'>>> 总体上来说很棒。这个宾馆比较陈旧了，特价的房间也很一般。总体来说一般。'\n",
      "'>>> 总体上来说很差。这个宾馆比较陈旧了，特价的房间也很一般。总体来说一般。'\n",
      "'>>> 总体上来说很般。这个宾馆比较陈旧了，特价的房间也很一般。总体来说一般。'\n",
      "'>>> 总体上来说很赞。这个宾馆比较陈旧了，特价的房间也很一般。总体来说一般。'\n"
     ]
    }
   ],
   "source": [
    "# Find the location of [MASK] and extract its logits\n",
    "mask_token_index = torch.where(inputs[\"input_ids\"] == tokenizer.mask_token_id)[1]\n",
    "mask_token_logits = token_logits[0, mask_token_index, :]\n",
    "# Pick the [MASK] candidates with the highest logits\n",
    "top_5_tokens = torch.topk(mask_token_logits, 5, dim=1).indices[0].tolist()\n",
    "\n",
    "for token in top_5_tokens:\n",
    "    print(f\"'>>> {text.replace(tokenizer.mask_token, tokenizer.decode([token]))}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "915729c5",
   "metadata": {},
   "source": [
    "当然，这种方式不够灵活，因此像之前章节中一样，本文采用继承 Transformers 库预训练模型的方式来手工构建模型："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "91560f4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "BertForPrompt(\n",
      "  (bert): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(21128, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (cls): BertOnlyMLMHead(\n",
      "    (predictions): BertLMPredictionHead(\n",
      "      (transform): BertPredictionHeadTransform(\n",
      "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (transform_act_fn): GELUActivation()\n",
      "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      )\n",
      "      (decoder): Linear(in_features=768, out_features=21128, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "from transformers.activations import ACT2FN\n",
    "from transformers import AutoConfig\n",
    "from transformers import BertPreTrainedModel, BertModel\n",
    "\n",
    "def batched_index_select(input, dim, index):\n",
    "    for i in range(1, len(input.shape)):\n",
    "        if i != dim:\n",
    "            index = index.unsqueeze(i)\n",
    "    expanse = list(input.shape)\n",
    "    expanse[0] = -1\n",
    "    expanse[dim] = -1\n",
    "    index = index.expand(expanse)\n",
    "    return torch.gather(input, dim, index)\n",
    "\n",
    "class BertPredictionHeadTransform(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        if isinstance(config.hidden_act, str):\n",
    "            self.transform_act_fn = ACT2FN[config.hidden_act]\n",
    "        else:\n",
    "            self.transform_act_fn = config.hidden_act\n",
    "        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "\n",
    "    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = self.transform_act_fn(hidden_states)\n",
    "        hidden_states = self.LayerNorm(hidden_states)\n",
    "        return hidden_states\n",
    "\n",
    "class BertLMPredictionHead(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.transform = BertPredictionHeadTransform(config)\n",
    "        self.decoder = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n",
    "        self.bias = nn.Parameter(torch.zeros(config.vocab_size))\n",
    "        self.decoder.bias = self.bias\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        hidden_states = self.transform(hidden_states)\n",
    "        hidden_states = self.decoder(hidden_states)\n",
    "        return hidden_states\n",
    "\n",
    "class BertOnlyMLMHead(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.predictions = BertLMPredictionHead(config)\n",
    "\n",
    "    def forward(self, sequence_output: torch.Tensor) -> torch.Tensor:\n",
    "        prediction_scores = self.predictions(sequence_output)\n",
    "        return prediction_scores\n",
    "\n",
    "class BertForPrompt(BertPreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.bert = BertModel(config, add_pooling_layer=False)\n",
    "        self.cls = BertOnlyMLMHead(config)\n",
    "        # Initialize weights and apply final processing\n",
    "        self.post_init()\n",
    "    \n",
    "    def get_output_embeddings(self):\n",
    "        return self.cls.predictions.decoder\n",
    "\n",
    "    def set_output_embeddings(self, new_embeddings):\n",
    "        self.cls.predictions.decoder = new_embeddings\n",
    "    \n",
    "    def forward(self, batch_inputs, batch_mask_idxs, label_word_id, labels=None):\n",
    "        bert_output = self.bert(**batch_inputs)\n",
    "        sequence_output = bert_output.last_hidden_state\n",
    "        batch_mask_reps = batched_index_select(sequence_output, 1, batch_mask_idxs.unsqueeze(-1)).squeeze(1)\n",
    "        pred_scores = self.cls(batch_mask_reps)[:, label_word_id]\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fn = nn.CrossEntropyLoss()\n",
    "            loss = loss_fn(pred_scores, labels)\n",
    "        return loss, pred_scores\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'Using {device} device')\n",
    "config = AutoConfig.from_pretrained(checkpoint)\n",
    "model = BertForPrompt.from_pretrained(checkpoint, config=config).to(device)\n",
    "if vtype == 'virtual':\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    print(f\"initialize embeddings of {verbalizer['pos']['token']} and {verbalizer['neg']['token']}\")\n",
    "    with torch.no_grad():\n",
    "        pos_tokenized = tokenizer(verbalizer['pos']['description'])\n",
    "        pos_tokenized_ids = tokenizer.convert_tokens_to_ids(pos_tokenized)\n",
    "        neg_tokenized = tokenizer(verbalizer['neg']['description'])\n",
    "        neg_tokenized_ids = tokenizer.convert_tokens_to_ids(neg_tokenized)\n",
    "        new_embedding = model.bert.embeddings.word_embeddings.weight[pos_tokenized_ids].mean(axis=0)\n",
    "        model.bert.embeddings.word_embeddings.weight[pos_id, :] = new_embedding.clone().detach().requires_grad_(True)\n",
    "        new_embedding = model.bert.embeddings.word_embeddings.weight[neg_tokenized_ids].mean(axis=0)\n",
    "        model.bert.embeddings.word_embeddings.weight[neg_id, :] = new_embedding.clone().detach().requires_grad_(True)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a03693",
   "metadata": {},
   "source": [
    "为了测试模型的操作是否符合预期，我们尝试将一个 batch 的数据送入模型："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "1d1188c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 2])\n"
     ]
    }
   ],
   "source": [
    "def to_device(batch_data):\n",
    "    new_batch_data = {}\n",
    "    for k, v in batch_data.items():\n",
    "        if k == 'batch_inputs':\n",
    "            new_batch_data[k] = {\n",
    "                k_: v_.to(device) for k_, v_ in v.items()\n",
    "            }\n",
    "        elif k == 'label_word_id':\n",
    "            new_batch_data[k] = v\n",
    "        else:\n",
    "            new_batch_data[k] = torch.tensor(v).to(device)\n",
    "    return new_batch_data\n",
    "\n",
    "batch_data = next(iter(train_dataloader))\n",
    "batch_data = to_device(batch_data)\n",
    "_, outputs = model(**batch_data)\n",
    "print(outputs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a428bd6",
   "metadata": {},
   "source": [
    "## **优化模型参数**\n",
    "\n",
    "我们将每一轮 Epoch 分为“训练循环”和“验证/测试循环”，在训练循环中计算损失、优化模型参数，在验证/测试循环中评估模型性能。下面我们首先实现训练循环。\n",
    "\n",
    "因为对标签词的预测实际上就是对类别的预测，损失是通过在类别预测和答案标签之间计算交叉熵："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "c7ce3b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "def train_loop(dataloader, model, optimizer, lr_scheduler, epoch, total_loss):\n",
    "    progress_bar = tqdm(range(len(dataloader)))\n",
    "    progress_bar.set_description(f'loss: {0:>7f}')\n",
    "    finish_step_num = epoch * len(dataloader)\n",
    "    \n",
    "    model.train()\n",
    "    for step, batch_data in enumerate(dataloader, start=1):\n",
    "        batch_data = to_device(batch_data)\n",
    "        outputs = model(**batch_data)\n",
    "        loss = outputs[0]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        progress_bar.set_description(f'loss: {total_loss/(finish_step_num + step):>7f}')\n",
    "        progress_bar.update(1)\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2fe7d22",
   "metadata": {},
   "source": [
    "借助机器学习包 sklearn 提供的 `classification_report` 函数来输出这些指标，例如："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "6c18081b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.75      0.60         4\n",
      "           1       0.67      0.57      0.62         7\n",
      "           2       1.00      0.50      0.67         2\n",
      "\n",
      "    accuracy                           0.62        13\n",
      "   macro avg       0.72      0.61      0.63        13\n",
      "weighted avg       0.67      0.62      0.62        13\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_true = [1, 1, 0, 1, 2, 1, 0, 2, 1, 1, 0, 1, 0]\n",
    "y_pred = [1, 0, 0, 1, 2, 0, 1, 1, 1, 0, 0, 1, 0]\n",
    "\n",
    "print(classification_report(y_true, y_pred, output_dict=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c70bc26",
   "metadata": {},
   "source": [
    "因此在验证/测试循环中，我们只需要汇总模型对所有样本的预测结果和答案标签，然后送入到 `classification_report` 中计算各项分类指标："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "07bfe0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def test_loop(dataloader, model):\n",
    "    true_labels, predictions = [], []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch_data in tqdm(dataloader):\n",
    "            true_labels += batch_data['labels']\n",
    "            batch_data = to_device(batch_data)\n",
    "            outputs = model(**batch_data)\n",
    "            pred = outputs[1]\n",
    "            predictions += pred.argmax(dim=-1).cpu().numpy().tolist()\n",
    "    metrics_text = classification_report(true_labels, predictions, target_names=['NEG', 'POS'], digits=4)\n",
    "    metrics_dict = classification_report(true_labels, predictions, output_dict=True)\n",
    "    print(metrics_text)\n",
    "    return metrics_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d76fcf",
   "metadata": {},
   "source": [
    "在开始训练之前，我们先评估一下没有微调的 BERT 模型在测试集上的性能。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "8a163a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_data = ChnSentiCorp('/kaggle/input/chnsenticorp-alllabeled/test.txt')\n",
    "# test_dataloader = DataLoader(test_data, batch_size=4, shuffle=False, collate_fn=collote_fn)\n",
    "\n",
    "# test_loop(test_dataloader, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40569221",
   "metadata": {},
   "source": [
    "## **训练&保存模型**\n",
    "\n",
    "我们会根据模型在验证集上的性能来调整超参数以及选出最好的模型权重，然后将选出的模型应用于测试集以评估最终的性能。这里我们继续使用 AdamW 优化器，并且通过 `get_scheduler()` 函数定义学习率调度器："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0163bd68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/a123/miniforge3/lib/python3.9/site-packages/transformers/optimization.py:457: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.002174854278564453,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 24,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 1200,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5e69a7709664f8bb1a7ebe594baad1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AdamW, get_scheduler\n",
    "\n",
    "learning_rate = 1e-5\n",
    "epoch_num = 5\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=epoch_num*len(train_dataloader),\n",
    ")\n",
    "\n",
    "total_loss = 0.\n",
    "best_f1_score = 0.\n",
    "for epoch in range(epoch_num):\n",
    "    print(f\"Epoch {epoch+1}/{epoch_num}\\n\" + 30 * \"-\")\n",
    "    total_loss = train_loop(train_dataloader, model, optimizer, lr_scheduler, epoch, total_loss)\n",
    "    valid_scores = test_loop(valid_dataloader, model)\n",
    "    macro_f1, micro_f1 = valid_scores['macro avg']['f1-score'], valid_scores['weighted avg']['f1-score']\n",
    "    f1_score = (macro_f1 + micro_f1) / 2\n",
    "    if f1_score > best_f1_score:\n",
    "        best_f1_score = f1_score\n",
    "        print('saving new weights...\\n')\n",
    "        torch.save(\n",
    "            model.state_dict(), \n",
    "            f'epoch_{epoch+1}_valid_macrof1_{(macro_f1*100):0.3f}_microf1_{(micro_f1*100):0.3f}_model_weights.bin'\n",
    "        )\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23897557",
   "metadata": {},
   "source": [
    "查看保存的权重信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e572378f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "best_weight_path = ''\n",
    "file_list = []\n",
    "\n",
    "# 遍历目录下的所有文件\n",
    "for dirname, _, filenames in os.walk('/kaggle/working'):\n",
    "    for filename in filenames:\n",
    "        # 构建文件的完整路径\n",
    "        file_path = os.path.join(dirname, filename)\n",
    "        \n",
    "        # 获取文件的最后修改时间\n",
    "        file_mtime = os.path.getmtime(file_path)\n",
    "        \n",
    "        # 将文件路径和最后修改时间添加到列表\n",
    "        file_list.append((file_path, file_mtime))\n",
    "\n",
    "# 按最后修改时间对文件列表进行排序\n",
    "sorted_files = sorted(file_list, key=lambda x: x[1])\n",
    "\n",
    "# 输出排序后的文件列表\n",
    "for file_path, file_mtime in sorted_files:\n",
    "    print(f\"{file_path} - Last Modified Time: {file_mtime}\")\n",
    "    \n",
    "best_weight_path = sorted_files[-1][0]\n",
    "\n",
    "print('best_weight_path :', best_weight_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ae6eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de5051d",
   "metadata": {},
   "source": [
    "# **3. 测试模型**\n",
    "\n",
    "训练完成后，我们加载在验证集上性能最优的模型权重，汇报其在测试集上的性能。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e7e3c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "model.load_state_dict(torch.load(best_weight_path))\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    print('evaluating on test set...')\n",
    "    true_labels, predictions, probs = [], [], []\n",
    "    for batch_data in tqdm(test_dataloader):\n",
    "        true_labels += batch_data['labels']\n",
    "        batch_data = to_device(batch_data)\n",
    "        outputs = model(**batch_data)\n",
    "        pred = outputs[1]\n",
    "        predictions += pred.argmax(dim=-1).cpu().numpy().tolist()\n",
    "        probs += torch.nn.functional.softmax(pred, dim=-1)\n",
    "    save_resluts = []\n",
    "    for s_idx in tqdm(range(len(test_data))):\n",
    "        save_resluts.append({\n",
    "            \"comment\": test_data[s_idx]['comment'], \n",
    "            \"label\": true_labels[s_idx], \n",
    "            \"pred\": predictions[s_idx], \n",
    "            \"prob\": {'neg': probs[s_idx][0].item(), 'pos': probs[s_idx][1].item()}\n",
    "        })\n",
    "        \n",
    "    metrics_text = classification_report(true_labels, predictions, target_names=['NEG', 'POS'], digits=4)\n",
    "    metrics_dict = classification_report(true_labels, predictions, output_dict=True)\n",
    "    print(metrics_text)\n",
    "#     metrics = classification_report(true_labels, predictions, output_dict=True)\n",
    "#     pos_p, pos_r, pos_f1 = metrics['1']['precision'], metrics['1']['recall'], metrics['1']['f1-score']\n",
    "#     neg_p, neg_r, neg_f1 = metrics['0']['precision'], metrics['0']['recall'], metrics['0']['f1-score']\n",
    "#     macro_f1, micro_f1 = metrics['macro avg']['f1-score'], metrics['weighted avg']['f1-score']\n",
    "#     print(f\"pos: {pos_p*100:>0.2f} / {pos_r*100:>0.2f} / {pos_f1*100:>0.2f}, neg: {neg_p*100:>0.2f} / {neg_r*100:>0.2f} / {neg_f1*100:>0.2f}\")\n",
    "#     print(f\"Macro-F1: {macro_f1*100:>0.2f} Micro-F1: {micro_f1*100:>0.2f}\\n\")\n",
    "    print('saving predicted results...')\n",
    "    with open('test_data_pred.json', 'wt', encoding='utf-8') as f:\n",
    "        for example_result in save_resluts:\n",
    "            f.write(json.dumps(example_result, ensure_ascii=False) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8f7e65",
   "metadata": {},
   "source": [
    "我们打开保存预测结果的 `test_data_pred.json`，其中每一行对应一个样本，`comment` 对应评论，`label` 对应标注标签，`pred` 对应预测出的标签，`prediction` 对应具体预测出的概率值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6281130",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 打开文件并加载JSON数据\n",
    "with open('test_data_pred.json', 'rt', encoding='utf-8') as file:\n",
    "    # 一次性读取所有行，并解析为JSON对象列表\n",
    "    json_data_list = [json.loads(line.strip()) for line in file]\n",
    "\n",
    "# 打印读取的JSON数据前五条\n",
    "for index, example_result in enumerate(json_data_list[:5], start=1):\n",
    "    json_str = json.dumps(example_result, ensure_ascii=False, indent=4)\n",
    "    print(json_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e79e7f7b",
   "metadata": {},
   "source": [
    "# **4. 封装预测函数**\n",
    "\n",
    "我们训练模型的目的是为了能够给其他人提供服务。尤其对于不熟悉深度学习的普通开发者而言，需要的只是一个能够完成特定任务的接口。因此在大多数情况下，我们都应该将模型的预测过程封装为一个端到端 (End-to-End) 的函数：输入文本，输出结果："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a0038c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, tokenizer, comment, verbalizer):\n",
    "    prompt_data = get_prompt(comment)\n",
    "    prompt = prompt_data['prompt']\n",
    "    encoding = tokenizer(prompt, truncation=True)\n",
    "    mask_idx = encoding.char_to_token(prompt_data['mask_offset'])\n",
    "    assert mask_idx is not None\n",
    "    inputs = tokenizer(\n",
    "        prompt, \n",
    "        max_length=max_length, \n",
    "        padding=True, \n",
    "        truncation=True, \n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    inputs = {\n",
    "        'batch_inputs': inputs, \n",
    "        'batch_mask_idxs': [mask_idx], \n",
    "        'label_word_id': [verbalizer['neg']['id'], verbalizer['pos']['id']] \n",
    "    }\n",
    "    inputs = to_device(inputs)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs[1]\n",
    "        prob = torch.nn.functional.softmax(logits, dim=-1)\n",
    "    pred = logits.argmax(dim=-1)[0].item()\n",
    "    prob = prob[0][pred].item()\n",
    "    return pred, prob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b74c9ef9",
   "metadata": {},
   "source": [
    "下面我们尝试输出模型对测试集前 5 条数据的预测结果："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70974e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(best_weight_path))\n",
    "\n",
    "for i in range(5):\n",
    "    data = test_data[i]\n",
    "    pred, prob = predict(model, tokenizer, data['comment'], verbalizer)\n",
    "    print(f\"{data['comment']}\\nlabel: {data['label']}\\tpred: {pred}\\tprob: {prob}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
